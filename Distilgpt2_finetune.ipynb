{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31091,
     "status": "ok",
     "timestamp": 1749710700828,
     "user": {
      "displayName": "elias samar",
      "userId": "15061332432326846285"
     },
     "user_tz": -210
    },
    "id": "A9PNShA_raRo",
    "outputId": "1a24073f-fc91-448b-ad64-042e9ab7566f"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUMaILAQx73t"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"hf_token\")  # Replace with your actual key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFuRYMhZGIZc"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate einops sentencepiece bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18052,
     "status": "ok",
     "timestamp": 1749543316153,
     "user": {
      "displayName": "elias samar",
      "userId": "15061332432326846285"
     },
     "user_tz": -210
    },
    "id": "9vFl-XhjhRfl",
    "outputId": "71b8fdb7-ee61-4ffd-c76a-827c03add825"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HhkOZhzYgVz1"
   },
   "outputs": [],
   "source": [
    "!pip install transformers accelerate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-3KQRe5KQK3"
   },
   "outputs": [],
   "source": [
    "!pip install transformers peft accelerate bitsandbytes datasets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZ56bYna9rxw"
   },
   "outputs": [],
   "source": [
    "!pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCq8tQUN-iAO"
   },
   "outputs": [],
   "source": [
    "!pip install bitsandbytes\n",
    "!pip install accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGGUStQhBdzp"
   },
   "outputs": [],
   "source": [
    "pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 419,
     "status": "ok",
     "timestamp": 1749629263376,
     "user": {
      "displayName": "elias samar",
      "userId": "15061332432326846285"
     },
     "user_tz": -210
    },
    "id": "8Ccyr-gBBfBo",
    "outputId": "3e9b24bf-6097-416e-c47e-1799d0eefcaf"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmWh2riHB78R"
   },
   "outputs": [],
   "source": [
    "pip install --upgrade bitsandbytes --extra-index-url https://pypi.org/simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1749629499018,
     "user": {
      "displayName": "elias samar",
      "userId": "15061332432326846285"
     },
     "user_tz": -210
    },
    "id": "k3PVW12FCMHg",
    "outputId": "e05c8c9a-0454-4690-d123-243c62c61479"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "print(\"Torch CUDA version:\", torch.version.cuda)\n",
    "print(\"BitsAndBytes version:\", bnb.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tiCJy6FCTHO"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y bitsandbytes\n",
    "!pip install --no-cache-dir bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Zgxb7uPCpO9"
   },
   "outputs": [],
   "source": [
    "pip install --upgrade torch transformers bitsandbytes accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⬆️ Upgrade bitsandbytes and install dependencies\n",
    "!pip install -U bitsandbytes\n",
    "!pip install -U transformers accelerate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297,
     "referenced_widgets": [
      "9831e417e88a4593aacc5fa099fe91eb",
      "eaa0bb7e354f496b9a47878f37187a9d",
      "274b98bd24164591ab673214a48a74d9",
      "420710e21ef14b928f35d66117735696",
      "a54b07c767924771918b967131699934",
      "c0b40844134f41aab0ee5863a380b57d",
      "4275fbdd01d94e4baf2942cc0651ee59",
      "fb301ddd4ce546cabec66b91c703887e",
      "5b2ee06a72ab489c8faec25077c9fa3a",
      "72f997a831074297a42d7043fcb1d4c8",
      "350fd47478574605bff10dfb6660f432",
      "88a2e3efa13546b8804f1af0acf6d54b",
      "56de564f914d4776a8d158f0f6527496",
      "90663b88625c460d9270bdd733f95143",
      "f1c1c518e96245d984e2c74b86f00f6a",
      "1931ef93a59943958d06fd0779d0cb1d",
      "dc4b96e81853429293fd3cc411ad1796",
      "ea887d8fe5ea411fae642e994e8f2935",
      "b71909d2e61f4626bf9c1984b6b7b902",
      "76bc5e8e3b144df1be58e1519e8893f5",
      "a00f466aa06743f0a52eb62b41e8b1b6",
      "518f04aad08843b69faef5f566d038a6",
      "0c4e0dfad2604fe4a55137ef3fe1e75b",
      "c73932da809942a0a0426cc29ac8422f",
      "b6c44fb6833a4257a51768129eb3f6a7",
      "dc23f73597e546dc8b04321d48e853e0",
      "9c0aa0a576c24491b0a4832a49c08df8",
      "7dede226f3bd48fe84f0b6ad74959cb8",
      "011260db08f849b5b9a4da9ebd8825e0",
      "75e0afd2c25542c68bba573de6e4f87b",
      "00f8c31ed82441b0a35b283db0875c40",
      "c4d416b526824cbb9a48a6f143fb914e",
      "83d18dcb6deb4df68346918accf5c926",
      "99761a4c246244429b2f6f3a219274a1",
      "82a6ebe1c8824a12a16ca4c3052f6d65",
      "43990af330f94a028689b0d408e823b4",
      "e22d72555baf44f286d2a7b5c5c1fab2",
      "ac89238484f14453b0414283e2b19d79",
      "a966f2bf972242df93ef23d6a4b443ee",
      "4703613da9974ff494a196f5e172518a",
      "22867744305649c19e2371b041d6d605",
      "5ce06e1d173c40b0b7052757a3a2d693",
      "e7b304506cd94adb9a00648edc8cd6b0",
      "77e2979112094c64be65475953275d87",
      "b0ad34c98ac04af0bbd3f1c99edbdf2d",
      "e6cf020b5b7c4ddcac37a2b95a64077f",
      "382bd9375ada4f2499ba1d4256660b1d",
      "afdde26b69744f0482556f95e7faa841",
      "1b7bed9abf8e40ef98b33ba1e3a710f6",
      "cb22935c885f4387a199171d84728643",
      "bb05165d50554776a27c937bff661f11",
      "8eb3f6500ae946bb965ec6d17a1f3f7f",
      "52e23a381c6e49a2be5dbcdfc313653c",
      "3784e5000c534a4f82359d2ac50f84dd",
      "9f0ba54df44648eb9eee34022d5e9522",
      "ef1ca4afaa304cfd8c33f9481c1ab79a",
      "8d905a9975f44f2a8cc1596101465cfb",
      "755f5d5aaa4d4b6ea7c5e0e58b9b423d",
      "1122c3864b3b481ebee68ce056e4913f",
      "5a3029ead98342e9a23e94609d689087",
      "e95859caa7744f4aa7a82bc52bcb29ce",
      "1e3f29612d7f424297e3fa51ac60fa31",
      "5d04e1a3953c41f5a4221399faf8dc47",
      "2bc2eb745bf0414cad0f111603cabf48",
      "9b40f7ea53734f7398c219018b8ff9d5",
      "7b1baf464f0441ed9bc86346124b1ef2",
      "6b59320fba1f4400b1362943d31c0a66",
      "ac8ebacf6e094128b61edf4246176629",
      "be48bd6b096c4c4886ecfe95f4f78114",
      "538482a404f24070b6e7b2b7a927bd56",
      "9eaa1785cf244ce4ba4d70d990943955",
      "fc008e3226144be296e27f8be2ca0002",
      "29e7c767d0274695855f153503be8a13",
      "19b1a5ecc87a498fbef2268226efd816",
      "00a25037ace44447afe3acac039208fd",
      "e40420e789e246329d59af6014c37477",
      "8724d4bf65aa42b2a15d6c22754b5dee"
     ]
    },
    "executionInfo": {
     "elapsed": 25933,
     "status": "ok",
     "timestamp": 1749634690352,
     "user": {
      "displayName": "elias samar",
      "userId": "15061332432326846285"
     },
     "user_tz": -210
    },
    "id": "4mZNfFABCp16",
    "outputId": "b8cd444f-045d-4c78-dd38-5f92cd0fa5eb"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set save path in your Google Drive\n",
    "save_path = \"/content/drive/MyDrive/distilgpt2\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Save tokenizer and model to Google Drive\n",
    "tokenizer.save_pretrained(save_path)\n",
    "model.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486,
     "referenced_widgets": [
      "a085be0714d44fc0bd7ae3c2b9ffad50",
      "0279eea9d1a941308c943bd06caf0e41",
      "8fa10dacdbe5487e9c74d2a408042950",
      "d59261aedb39439f8d138fe4109d1831",
      "d8b71892289949d187d0ab5e6f280b02",
      "4bd650fd209f4e8787c9941e7617ff7a",
      "113858c22f584c8d92a195e198fbbac1",
      "6163b9593ff24804ab89146380489b33",
      "6d68ea5a1d9742198c64a249630e0012",
      "da23ef0d68e14959bba09fc994ae0531",
      "be9f14f23b264f69be90691ef5f7dc2d",
      "52c5b5aa5e024938a49945aa605ec84c",
      "d00e4bf3ae364f7bab867be3e5f5a7bf",
      "3a62c53c72c246029e35ce970240750a",
      "e16d7655b5a84f0f9ab1f3830aa50588",
      "c3e0500054184b149aabbf5601b9977b",
      "7bd67d3c23ac4c74a3ca9616b3c1375d",
      "aa39ffb1486243e2a1c1d903b56211bc",
      "b4b12d3eed4d4d2c9c294c3c0cfe4676",
      "a982af8012c645d0aa3f415e95792ad0",
      "9efb5d2c1fe6475bbc5579e180ed91f9",
      "93c608062be6419091b8b0b638514ea0"
     ]
    },
    "executionInfo": {
     "elapsed": 214524,
     "status": "ok",
     "timestamp": 1749711794392,
     "user": {
      "displayName": "elias samar",
      "userId": "15061332432326846285"
     },
     "user_tz": -210
    },
    "id": "vvWgtA49XHGP",
    "outputId": "1e106db6-f375-45af-969d-412fa750325a"
   },
   "outputs": [],
   "source": [
    "#main\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# ✅ Paths\n",
    "model_name = \"distilgpt2\"\n",
    "train_csv = \"/content/drive/MyDrive/llm-classification-finetuning/train.csv\"\n",
    "output_dir = \"/content/drive/MyDrive/llm-classification-finetuning/distilgpt2-finetuned\"\n",
    "\n",
    "# ✅ Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ✅ Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# ✅ Load and sample dataset\n",
    "df = pd.read_csv(train_csv)\n",
    "df = df.sample(n=500, random_state=42)  # ← Adjust as needed for faster runs\n",
    "\n",
    "# ✅ Prompt formatting\n",
    "def format_prompt(batch):\n",
    "    texts = []\n",
    "    for prompt, resp_a, resp_b, winner_a, winner_b in zip(\n",
    "        batch[\"prompt\"], batch[\"response_a\"], batch[\"response_b\"],\n",
    "        batch[\"winner_model_a\"], batch[\"winner_model_b\"]\n",
    "    ):\n",
    "        prompt_text = f\"\"\"A user asked the following question:\n",
    "\"{prompt}\"\n",
    "\n",
    "Two different AI assistants replied:\n",
    "\n",
    "Response A:\n",
    "\"{resp_a}\"\n",
    "\n",
    "Response B:\n",
    "\"{resp_b}\"\n",
    "\n",
    "Which response is more helpful, human-like, and aligned with user expectations? Reply with only \"A\" or \"B\".\n",
    "\"\"\"\n",
    "        label = \"A\" if winner_a == 1 else (\"B\" if winner_b == 1 else \"C\")\n",
    "        texts.append(prompt_text + label)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# ✅ Hugging Face dataset + formatting\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.map(format_prompt, batched=True)\n",
    "\n",
    "# ✅ Tokenization\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, remove_columns=dataset.column_names, batched=True)\n",
    "\n",
    "# ✅ Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# ✅ Training arguments with checkpoint saving\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=46,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"steps\",       # Save regularly for resuming\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# ✅ Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ✅ Resume from checkpoint if available\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(output_dir):\n",
    "    checkpoints = [os.path.join(output_dir, d) for d in os.listdir(output_dir) if d.startswith(\"checkpoint\")]\n",
    "    if checkpoints:\n",
    "        last_checkpoint = max(checkpoints, key=os.path.getmtime)\n",
    "        print(f\"🔁 Resuming from checkpoint: {last_checkpoint}\")\n",
    "\n",
    "# ✅ Train\n",
    "start = time.time()\n",
    "trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "print(f\"⏱️ Training finished in {(time.time() - start)/60:.2f} minutes\")\n",
    "\n",
    "# ✅ Save final model\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"✅ Finetuning complete — model saved to Google Drive.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XlfCxWHDntc"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOamWu5V5/MSH5J6dh9/sKf",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
