{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2206,
     "status": "ok",
     "timestamp": 1751104096198,
     "user": {
      "displayName": "shima khosravani",
      "userId": "16013126042930970123"
     },
     "user_tz": -210
    },
    "id": "H3L7AvHVVe4R"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"hf_token\")  # Replace with your actual key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51233,
     "status": "ok",
     "timestamp": 1751104086176,
     "user": {
      "displayName": "shima khosravani",
      "userId": "16013126042930970123"
     },
     "user_tz": -210
    },
    "id": "4RlXXWC4VGla",
    "outputId": "67537832-d849-4fd4-94ce-38b415fe17c0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf1tbiodTriA"
   },
   "outputs": [],
   "source": [
    "# üì¶ Install required libraries\n",
    "!pip install -U bitsandbytes transformers datasets peft accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XoHe2IHYiAX"
   },
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes\n",
    "!pip install -U transformers accelerate datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LN-bT2MLuhx3"
   },
   "outputs": [],
   "source": [
    "!apt-get install -y git-lfs\n",
    "!git lfs install\n",
    "!git clone https://huggingface.co/unsloth/Llama-3.2-1B-Instruct /content/drive/MyDrive/llama_models/Llama-3.2-1B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569,
     "referenced_widgets": [
      "8aa69f13a0f040e9a4751cc1e2f0d472",
      "6980b78c351249d8ae9234ab7fafeb57",
      "c278ca5ea2dc478e91735ab1ead715ce",
      "fd5411b432e44dd09ff44b6c1f552405",
      "4dec4360d84b47889a819e4ccd2257bb",
      "3677f8e17ca94bec8dd28f51f1650f23",
      "c10ece9c58eb484da490273eed62cee2",
      "3ab26bb4dc124d8bbab45a226a0838f8",
      "6f18120bf46a4dc9ba10635e2d7f5a7d",
      "36d25a63830f4ea5a6ffc41c783d4337",
      "66250690727b4280b5e8cba9927599e0",
      "7921af0ca4a84b10bd8795accdfe286e",
      "4651a09ecd77428996a415f51cd1214b",
      "9338d3b7a88448d79edc5042e52d125d",
      "e5e4fc291b0f4a5db16fb82de9e96c90",
      "1ae8c021bdc744e49d698ec2dc266b0d",
      "8b731b478e654968b0cac0c124c7fc40",
      "454d13a28a784331b1c71befb3607d88",
      "ba14a57e42d54e34815d47a65e12e776",
      "73ac2a463846463884196f1abcfaf7de",
      "98ba08e15a4f412db44be134c6d78317",
      "5239eb5a275f41558db5863a0764b977"
     ]
    },
    "id": "W3sZ-8iNVi70",
    "outputId": "2ca18139-7c5d-41c0-e4da-17d46cda3135"
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Imports\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainerCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# ‚úÖ VRAM monitoring callback\n",
    "class VRAMCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if torch.cuda.is_available():\n",
    "            vram_allocated = round(torch.cuda.memory_allocated() / 1e9, 2)\n",
    "            vram_reserved = round(torch.cuda.memory_reserved() / 1e9, 2)\n",
    "            vram_max_allocated = round(torch.cuda.max_memory_allocated() / 1e9, 2)\n",
    "            print(f\"[Memory] Allocated: {vram_allocated} GB | Reserved: {vram_reserved} GB | Max Allocated: {vram_max_allocated} GB\")\n",
    "\n",
    "# ‚úÖ Paths\n",
    "model_name = \"/content/drive/MyDrive/llama_models/Llama-3.2-1B-Instruct\"\n",
    "train_csv = \"/content/drive/MyDrive/llm-classification-finetuning/train.csv\"\n",
    "output_dir = \"/content/drive/MyDrive/llm-classification-finetuning/llama-lora-finetuned\"\n",
    "\n",
    "# ‚úÖ Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ‚úÖ Load 4-bit quantized model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# ‚úÖ Enable model for LoRA fine-tuning\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# ‚úÖ Add LoRA adapters\n",
    "peft_config = LoraConfig(\n",
    "    r=128,\n",
    "    lora_alpha=128,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        \"embed_tokens\", \"lm_head\"\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"all\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# ‚úÖ Load and format dataset\n",
    "df = pd.read_csv(train_csv)\n",
    "df = df.sample(n=500, random_state=42)\n",
    "\n",
    "def format_prompt(batch):\n",
    "    texts = []\n",
    "    for prompt, resp_a, resp_b, winner_a, winner_b in zip(\n",
    "        batch[\"prompt\"], batch[\"response_a\"], batch[\"response_b\"],\n",
    "        batch[\"winner_model_a\"], batch[\"winner_model_b\"]\n",
    "    ):\n",
    "        prompt_text = f\"\"\"A user asked the following question:\n",
    "\"{prompt}\"\n",
    "\n",
    "Two different AI assistants replied:\n",
    "\n",
    "Response A:\n",
    "\"{resp_a}\"\n",
    "\n",
    "Response B:\n",
    "\"{resp_b}\"\n",
    "\n",
    "Which response is more helpful, human-like, and aligned with user expectations? Reply with only \"A\" or \"B\".\n",
    "\"\"\"\n",
    "        label = \"A\" if winner_a == 1 else (\"B\" if winner_b == 1 else \"C\")\n",
    "        texts.append(prompt_text + label)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.map(format_prompt, batched=True)\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, remove_columns=dataset.column_names, batched=True)\n",
    "\n",
    "# ‚úÖ Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# ‚úÖ Training setup: save all checkpoints\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=15,\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"epoch\",   # ‚úÖ Save once per epoch\n",
    "    # ‚ùå save_total_limit removed ‚Üí keeps all\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# ‚úÖ Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[VRAMCallback()]\n",
    ")\n",
    "\n",
    "# ‚úÖ Check for last checkpoint\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(output_dir):\n",
    "    checkpoints = [\n",
    "        os.path.join(output_dir, d)\n",
    "        for d in os.listdir(output_dir)\n",
    "        if d.startswith(\"checkpoint\")\n",
    "    ]\n",
    "    if checkpoints:\n",
    "        last_checkpoint = max(checkpoints, key=os.path.getctime)\n",
    "        print(f\"‚úÖ Resuming from checkpoint: {last_checkpoint}\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è No checkpoint found, training from scratch.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No checkpoint folder found, training from scratch.\")\n",
    "\n",
    "# ‚úÖ Train (resume if checkpoint found)\n",
    "trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "\n",
    "# ‚úÖ Save final model & tokenizer\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112191,
     "status": "ok",
     "timestamp": 1751104287693,
     "user": {
      "displayName": "shima khosravani",
      "userId": "16013126042930970123"
     },
     "user_tz": -210
    },
    "id": "C5UwdR-n6lpe",
    "outputId": "3dfa5629-e7ff-4c56-d449-27930b3147b2"
   },
   "outputs": [],
   "source": [
    "#submission code\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# ‚úÖ Load local model\n",
    "model_path = \"/content/drive/MyDrive/llm-classification-finetuning/llama-lora-finetuned/checkpoint-208\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "# ‚úÖ Enable sampling to estimate probabilities\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=20,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# ‚úÖ Load dataset\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/llm-classification-finetuning/test.csv\")\n",
    "\n",
    "# ‚úÖ Prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "A user asked the following question:\n",
    "\"{prompt}\"\n",
    "\n",
    "Two different AI assistants replied:\n",
    "\n",
    "Response A:\n",
    "\"{response_a}\"\n",
    "\n",
    "Response B:\n",
    "\"{response_b}\"\n",
    "\n",
    "Which response is more helpful, human-like, and aligned with user expectations? Reply with only \"A\" or \"B\".\n",
    "\"\"\"\n",
    "\n",
    "# ‚úÖ Generate probabilistic predictions\n",
    "predictions = []\n",
    "\n",
    "N_SAMPLES = 5  # number of samples per input\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    prompt = row['prompt']\n",
    "    response_a = row['response_a']\n",
    "    response_b = row['response_b']\n",
    "    uid = row['id']\n",
    "\n",
    "    # Randomly flip A/B to reduce bias\n",
    "    if random.random() < 0.5:\n",
    "        actual_a, actual_b = response_a, response_b\n",
    "        flipped = False\n",
    "    else:\n",
    "        actual_a, actual_b = response_b, response_a\n",
    "        flipped = True\n",
    "\n",
    "    prompt_text = PROMPT_TEMPLATE.format(\n",
    "        prompt=prompt,\n",
    "        response_a=actual_a.strip(),\n",
    "        response_b=actual_b.strip()\n",
    "    )\n",
    "\n",
    "    results = generator(prompt_text, num_return_sequences=N_SAMPLES)\n",
    "\n",
    "    count_a = 0\n",
    "    count_b = 0\n",
    "\n",
    "    for res in results:\n",
    "        last_line = res['generated_text'].strip().split('\\n')[-1].strip().upper()\n",
    "        if \"B\" in last_line:\n",
    "            pred = \"B\"\n",
    "        else:\n",
    "            pred = \"A\"\n",
    "\n",
    "        # Undo flipping\n",
    "        if flipped:\n",
    "            pred = \"A\" if pred == \"B\" else \"B\"\n",
    "\n",
    "        if pred == \"A\":\n",
    "            count_a += 1\n",
    "        elif pred == \"B\":\n",
    "            count_b += 1\n",
    "\n",
    "    # Normalize to probabilities\n",
    "    total = count_a + count_b\n",
    "    prob_a = count_a / total\n",
    "    prob_b = count_b / total\n",
    "    prob_tie = 0.0  # no tie handling yet\n",
    "\n",
    "    predictions.append({\n",
    "        \"id\": uid,\n",
    "        \"winner_model_a\": prob_a,\n",
    "        \"winner_model_b\": prob_b,\n",
    "        \"winner_tie\": prob_tie\n",
    "    })\n",
    "\n",
    "# ‚úÖ Save to CSV in correct format\n",
    "out_df = pd.DataFrame(predictions)\n",
    "out_df.to_csv(\"/content/drive/MyDrive/llm-classification-finetuning/submission.csv\", index=False)\n",
    "print(\"‚úÖ Saved final submission with probabilities as submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMDP41xjZotwpTp8R8yRORi",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
